{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_reasoning_modules(reasoning_modules, task_example):\n",
    "    \"\"\"\n",
    "    Step 1: Adjust the selected reasoning module to make it more suitable for the task.\n",
    "    \"\"\"\n",
    "    prompt = f\"Without working out the full solution, adapt the following reasoning modules to be specific to our task:\\n{reasoning_modules}\\n\\nOur task:\\n{task_example}\\n\"\n",
    "    adapted_modules = query_openai(prompt)\n",
    "    return adapted_modules\n",
    "\n",
    "def implement_reasoning_structure(adapted_modules, task_description):\n",
    "    \"\"\"\n",
    "    Step 2: Implement the adjusted reasoning module into an operational reasoning structure.\n",
    "    \"\"\"\n",
    "    prompt = f\"Without working out the full solution, create an actionable reasoning structure for the task using these adapted reasoning modules:\\n{adapted_modules}\\n\\nTask Description:\\n{task_description}\"\n",
    "    reasoning_structure = query_openai(prompt)\n",
    "    return reasoning_structure\n",
    "\n",
    "def execute_reasoning_structure(reasoning_structure, task_example):\n",
    "    \"\"\"\n",
    "    Step 3: Execute the reasoning structure to solve a specific task instance.\n",
    "    \"\"\"\n",
    "    prompt = f\"Using the following reasoning structure: {reasoning_structure}\\n\\nSolve this problem {task_example} and develop a Python solution to this problem that obeys the constraints and passes the example test case. You only need to complete the following steps for now: 1. Break down the solution into modules with clear function names and input/output specifications 2. Outline the required code modules, including function headers and signatures. 3. Ensure modularity and consider potential edge cases and failures. 4. Once the structure is ready, write the actual code for each module.\"\n",
    "    solution = query_openai(prompt)\n",
    "    return solution\n",
    "\n",
    "def generate_code_with_reasoning(task_description, task_example, reasoning_modules):\n",
    "    \"\"\"\n",
    "    Integrate the reasoning module approach into the code generation framework.\n",
    "    \"\"\"\n",
    "    adapted_modules = adapt_reasoning_modules(reasoning_modules, task_example) \n",
    "    reasoning_structure = implement_reasoning_structure(adapted_modules, task_description)\n",
    "    result = execute_reasoning_structure(reasoning_structure, task_example)\n",
    "    print(result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fusion Retriever\n",
    "TOP_K = 16\n",
    "MAX_DOCS_FOR_CONTEXT = 4\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"Reciprocal Rank Fusion\n",
    "\n",
    "    Parameters:\n",
    "    results (list[list]): retrieved function blocks\n",
    "    k (int, optional): RRF parameter. Default value is 60.\n",
    "\n",
    "    Returns:\n",
    "    ranked_results: re-ranked function block list\n",
    "    \"\"\"\n",
    "    fused_scores = {}\n",
    "    for funcs in results:\n",
    "        for rank, func in enumerate(funcs):\n",
    "            func_str = dumps(func) \n",
    "            if func_str not in fused_scores:\n",
    "                fused_scores[func_str] = 0\n",
    "            fused_scores[func_str] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(func), score)\n",
    "        for func, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    return [x[0] for x in reranked_results[:MAX_DOCS_FOR_CONTEXT]]\n",
    "\n",
    "def create_retriever_2(search_type: str, kwargs: dict) -> BaseRetriever:\n",
    "    \"\"\"Create a function block vector retriever\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    search_type (str): search type\n",
    "    kwargs (dict): other parameters\n",
    "\n",
    "    Returns:\n",
    "    BaseRetriever: function block retriever\n",
    "    \"\"\"\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=\"Chroma_databases\",\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(search_type=search_type, search_kwargs=kwargs)\n",
    "    return retriever\n",
    "\n",
    "def query_generator(original_query: dict) -> list[str]:\n",
    "    \"\"\"Generate a query from a raw query\n",
    "\n",
    "    Parameters:\n",
    "    query (dict): raw query\n",
    "\n",
    "    Returns:\n",
    "    list[str]: generated query list\n",
    "    \"\"\"\n",
    "    query = original_query.get(\"query\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that can extract natural language parts from the input and generate multiple search queries based on them.\"),\n",
    "        (\"user\", \"Generate multiple search queries based on: {original_query}. When creating queries, refine or add closely related contextual information without significantly changing the meaning of the original query, in English.\"),\n",
    "        (\"user\", \"Output (5 queries):\")\n",
    "    ])\n",
    "    query_generator_chain = (\n",
    "        prompt | llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "    )\n",
    "    queries = query_generator_chain.invoke({\"original_query\": query})\n",
    "    queries.insert(0, \"0. \" + query)\n",
    "\n",
    "    return queries\n",
    "\n",
    "def rrf_retriever_2(query: str) -> list[Document]:\n",
    "    \"\"\"RRF function block retriever\n",
    "\n",
    "    Parameters:\n",
    "    query (str): query string\n",
    "\n",
    "    Returns:\n",
    "    list[Document]: retrieved function blocks\n",
    "    \"\"\"\n",
    "    retriever = create_retriever_2(search_type=\"similarity\", kwargs={\"k\": TOP_K})\n",
    "    chain = (\n",
    "        {\"query\": itemgetter(\"query\")}\n",
    "        | RunnableLambda(query_generator)\n",
    "        | retriever.map() \n",
    "        | reciprocal_rank_fusion\n",
    "    )\n",
    "    result = chain.invoke({\"query\": query})\n",
    "    print(result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_representative_submodules(submodules, n_representatives=5):\n",
    "    \"\"\"\n",
    "    Evaluate the generated code samples using the large model and select representative submodules.\n",
    "\n",
    "    submodules: List[str] Generated submodules.\n",
    "    n_representatives: int Number of representative submodules selected.\n",
    "\n",
    "    Returns the selected representative submodules.\n",
    "    \"\"\"\n",
    "    if not submodules:\n",
    "        logging.error(\"No submodules provided for evaluation.\")\n",
    "        raise ValueError(\"No submodules provided for evaluation.\")\n",
    "    \n",
    "    formatted_submodules = \"\\n\".join([f\"{i+1}. {submodule}\" for i, submodule in enumerate(submodules)])\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    You are given several code submodules extracted from a larger program. Your task is to evaluate these submodules and select the ones that are most representative of the overall structure and logic of the code. Choose up to {n_representatives} submodules that you believe would best represent the core functionality.\n",
    "\n",
    "    Submodules:\n",
    "    {formatted_submodules}\n",
    "\n",
    "    Please provide the index numbers of the most representative submodules.\n",
    "    \"\"\"\n",
    "\n",
    "    rendered_prompt = prompt_template.format(\n",
    "        n_representatives=n_representatives,\n",
    "        formatted_submodules=formatted_submodules\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(rendered_prompt)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    representative_indices = [int(idx) for idx in response_content.split() if idx.isdigit() and 1 <= int(idx) <= len(submodules)]\n",
    "\n",
    "    representative_submodules = [submodules[idx - 1] for idx in representative_indices if 0 <= idx - 1 < len(submodules)]\n",
    "    \n",
    "    return representative_submodules\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def extract_representative_submodules(submodules, n_representatives=5):\n",
    "    \"\"\"\n",
    "    Select representative submodules using similarity matrix.\n",
    "\n",
    "    submodules: List[str] Generated submodules.\n",
    "    n_representatives: int Number of selected representative submodules.\n",
    "\n",
    "    Returns the selected representative submodules.\n",
    "    \"\"\"\n",
    "    if not submodules:\n",
    "        logging.error(\"No submodules provided for evaluation.\")\n",
    "        raise ValueError(\"No submodules provided for evaluation.\")\n",
    "    \n",
    "    embeddings = embed_samples(submodules)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    representative_indices = []\n",
    "    for i in range(n_representatives):\n",
    "        max_similarity_index = np.argmax(np.sum(similarity_matrix, axis=1))\n",
    "        representative_indices.append(max_similarity_index)\n",
    "        similarity_matrix[max_similarity_index, :] = 0\n",
    "        similarity_matrix[:, max_similarity_index] = 0\n",
    "    \n",
    "    representative_submodules = [submodules[idx] for idx in representative_indices]\n",
    "    \n",
    "    return representative_submodules\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def extract_representative_submodules(submodules, n_representatives=5, lambda_=0.5):\n",
    "    \"\"\"\n",
    "    Combine similarity-based selection with LLM evaluation to select representative submodules.\n",
    "\n",
    "    submodules: List[str] Generated submodules.\n",
    "    n_representatives: int Number of representative submodules to select.\n",
    "    lambda_: float Balancing parameter for MMR (0 <= lambda_ <= 1).\n",
    "    \"\"\"\n",
    "    if not submodules:\n",
    "        logging.error(\"No submodules provided for evaluation.\")\n",
    "        return []\n",
    "    \n",
    "    if len(submodules) < n_representatives:\n",
    "        logging.warning(\"Number of submodules is less than n_representatives. Adjusting n_representatives.\")\n",
    "        n_representatives = len(submodules)\n",
    "    \n",
    "    # Step 1: Generate embeddings for submodules\n",
    "    embeddings = embed_samples(submodules)\n",
    "    \n",
    "    # Step 2: Compute similarity matrix\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Step 3: MMR-based selection\n",
    "    # Compute average similarity of each submodule to all others\n",
    "    average_similarity = np.mean(similarity_matrix, axis=1)\n",
    "    \n",
    "    selected = []\n",
    "    candidates = list(range(len(submodules)))\n",
    "    \n",
    "    # Select the first representative with highest average similarity\n",
    "    first_idx = np.argmax(average_similarity)\n",
    "    selected.append(first_idx)\n",
    "    candidates.remove(first_idx)\n",
    "    \n",
    "    for _ in range(1, n_representatives):\n",
    "        if not candidates:\n",
    "            logging.warning(\"No more candidates available to select.\")\n",
    "            break\n",
    "        mmr_scores = []\n",
    "        for idx in candidates:\n",
    "            # Similarity to the entire set\n",
    "            relevance = average_similarity[idx]\n",
    "            # Similarity to the selected set\n",
    "            similarity_to_selected = np.mean(similarity_matrix[idx, selected])\n",
    "            # MMR score\n",
    "            mmr = lambda_ * relevance - (1 - lambda_) * similarity_to_selected\n",
    "            mmr_scores.append(mmr)\n",
    "        \n",
    "        if not mmr_scores:\n",
    "            logging.warning(\"No mmr_scores available to select.\")\n",
    "            break\n",
    "        selected_idx = candidates[np.argmax(mmr_scores)]\n",
    "        selected.append(selected_idx)\n",
    "        candidates.remove(selected_idx)\n",
    "    \n",
    "    # Step 4: LLM evaluation to select final representatives\n",
    "    # Format the selected candidates\n",
    "    formatted_submodules = \"\\n\".join([f\"{i+1}. {submodules[idx]}\" for i, idx in enumerate(selected)])\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    You are given several code submodules extracted from a larger program. Your task is to evaluate these submodules and select the ones that are most representative of the overall structure and logic of the code. Choose up to {n_representatives} submodules that you believe would best represent the core functionality.\n",
    "\n",
    "    Submodules:\n",
    "    {formatted_submodules}\n",
    "\n",
    "    Please provide the index numbers of the most representative submodules.\n",
    "    \"\"\"\n",
    "    \n",
    "    rendered_prompt = prompt_template.format(\n",
    "        n_representatives=n_representatives,\n",
    "        formatted_submodules=formatted_submodules\n",
    "    )\n",
    "    \n",
    "    response = llm.invoke(rendered_prompt)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "    \n",
    "    # Extract indices selected by LLM\n",
    "    selected_indices = [int(idx) - 1 for idx in response_content.split() if idx.isdigit() and 1 <= int(idx) <= len(selected)]\n",
    "    \n",
    "    # Get the final representative submodules\n",
    "    representative_submodules = [submodules[selected[idx]] for idx in selected_indices]\n",
    "    \n",
    "    return representative_submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt_template = \"\"\"\n",
    "*Task*\n",
    "In the current task, you will be given [code block], [problem], and [reasoning solution]. For the given problem [problem], develop a well-structured Python solution, ensure modularity, and consider potential edge cases and failures.\n",
    "\n",
    "*Instructions*\n",
    "1. Give a reasoning solution based on [problem], with code for you to use\n",
    "2. Give a code block [code block] that may be related to [problem]\n",
    "3. You can try to use or adapt [code block] in completing or improving each module in [reasoning solution], or abandon [code block] and complete or improve each module in [reasoning solution] by yourself\n",
    "4. Complete or improve each module in [reasoning solution], write actual code for each module to complete the solution. Output submodules need to be packaged. Please wrap the code answer of each submodule with <<start_submodule>> and <<end_submodule>>\n",
    "\n",
    "*Output*\n",
    "Generated submodules: <<start_submodule>> submodule1<<end_submodule>><<start_submodule>> submodule2<<end_submodule>>, etc.\n",
    "\n",
    "*Input*\n",
    "Question: {question}\n",
    "Inference solution: {generated_code}\n",
    "Code block: {context}\n",
    "\"\"\"\n",
    "cot_prompt_template_2 =\"\"\"\n",
    "*Task*\n",
    "Develop a well-structured Python solution to the given question that adheres to the constraints. Ensure modularity and consider potential edge cases and failures.\n",
    "\n",
    "*Instructions*\n",
    "1. Create a clean, organized Python solution to the given question [question]. Break it down into several submodules with clear function names and input/output specifications\n",
    "2. Given a set of related useful Python functions [representing submodules], try to reuse or adapt them as much as possible to fit your solution in the next step (create new unique functions if needed).\n",
    "3. Once the structure is ready, write the actual code for each module to complete the solution.\n",
    "4. Output submodules need to be wrapped. Please wrap your code answer with <<start_submodule>> and <<end_submodule>>.\n",
    "\n",
    "*Output*\n",
    "Generated submodules: <<start_submodule>> submodule1 <<end_submodule>><<start_submodule>> submodule2 <<end_submodule>> etc.\n",
    "\n",
    "*Input*\n",
    "question: {question}\n",
    "Represented submodules: {submodules}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = HuggingFaceEmbeddings(model_name=embedding_path)\n",
    "vectorstore = Chroma(persist_directory=None, embedding_function=embedding_function)\n",
    "retriever = VectorStoreRetriever(vectorstore=vectorstore)\n",
    "\n",
    "\n",
    "def split_task_into_submodules(task_result):\n",
    "    \"\"\"\n",
    "    Split the task result into multiple submodule tasks.\n",
    "\n",
    "    task_result: str The final result of the task.\n",
    "\n",
    "    Return the list of submodule tasks after splitting\n",
    "    \"\"\"\n",
    "    prompt = f\"Analyze the provided content \\n{task_result}\\n\\n and identify subtasks under 'code module'. Extract each subtask starting with '#### N. module name' and separate the output with markers '### split ###'. Each subtask should include functional details and description.\"\n",
    "    submodules_text = query_openai(prompt)\n",
    "    print(f\"Split Submodules:\\n{submodules_text}\")\n",
    "\n",
    "    submodules = re.split(r'### split ###\\n\\n', submodules_text)\n",
    "    \n",
    "    # Exclude the first empty element\n",
    "    submodules = submodules[1:]\n",
    "    \n",
    "    # Print each submodule\n",
    "    for i, submodule in enumerate(submodules):\n",
    "        print(f\"Submodule {i+1}:\\n{submodule.strip()}\\n\")\n",
    "    \n",
    "    return submodules\n",
    "\n",
    "def refine_submodule(submodule, retriever, num_rounds=2, max_attempts=5):\n",
    "    \"\"\"\n",
    "    Perform multiple rounds of iterative refinement on a single submodule.\n",
    "\n",
    "    submodule: str Submodule task.\n",
    "    retriever: VectorStoreRetriever Retriever used for query.\n",
    "    num_rounds: int Number of rounds cot generated.\n",
    "    max_attempts: int Maximum number of generation attempts.\n",
    "\n",
    "    Return the refined submodule.\n",
    "    \"\"\"\n",
    "    refined_submodule = cot(submodule, submodule, retriever, num_rounds, max_attempts)\n",
    "    return refined_submodule\n",
    "\n",
    "def merge_submodules(submodules):\n",
    "    \"\"\"\n",
    "    Merge all submodule tasks together and remove duplicate submodules.\n",
    "\n",
    "    submodules: List[str] Submodule task list.\n",
    "\n",
    "    Return the final merged code.\n",
    "    \"\"\"\n",
    "\n",
    "    submodules = [str(submodule) for submodule in submodules]\n",
    "    unique_submodules = list(set(submodules))\n",
    "    merged_code = \"\\n\\n\".join(unique_submodules)\n",
    "    \n",
    "    return merged_code\n",
    "\n",
    "def embed_samples(samples):\n",
    "    if not samples:\n",
    "        logging.error(\"No samples provided for embedding.\")\n",
    "        raise ValueError(\"No samples provided for embedding.\")\n",
    "    \n",
    "    embeddings = embedding_function.embed_documents(samples)\n",
    "    vectorstore.add_texts(samples)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def extract_submodules_from_code(generated_code):\n",
    "    \"\"\"\n",
    "    Extract submodules from generated code.\n",
    "\n",
    "    generated_code: str Generated code string.\n",
    "\n",
    "    Returns a list of extracted submodules.\n",
    "    \"\"\"\n",
    "    submodules = []\n",
    "    start_token = \"<<start_submodule>>\"\n",
    "    end_token = \"<<end_submodule>>\"\n",
    "    \n",
    "    if start_token not in generated_code or end_token not in generated_code:\n",
    "        logging.warning(\"Start or end token not found in generated code.\")\n",
    "        return submodules\n",
    "    \n",
    "    start_idx = 0\n",
    "    while start_idx != -1:\n",
    "        start_idx = generated_code.find(start_token, start_idx)\n",
    "        if (start_idx == -1) or (start_idx + len(start_token) >= len(generated_code)):\n",
    "            break\n",
    "        end_idx = generated_code.find(end_token, start_idx)\n",
    "        if end_idx == -1:\n",
    "            break\n",
    "        submodule = generated_code[start_idx + len(start_token):end_idx].strip()\n",
    "        submodules.append(submodule)\n",
    "        start_idx = end_idx + len(end_token)\n",
    "    \n",
    "\n",
    "    submodules = list(set(submodules))\n",
    "    \n",
    "    return submodules\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_cot_prompt_with_submodules(cot_prompt, representative_submodules):\n",
    "    \"\"\"\n",
    "   Add representative submodules to the original thought chain prompt.\n",
    "\n",
    "   cot_prompt: str Original thought chain prompt.\n",
    "   representative_submodules: List[str] Extracted representative submodules.\n",
    "\n",
    "   Return the updated prompt.\n",
    "    \"\"\"\n",
    "    if not representative_submodules:\n",
    "        logging.warning(\"No representative submodules provided for updating prompt.\")\n",
    "        return cot_prompt\n",
    "    \n",
    "    template = Template(cot_prompt + \"\\n\\n# Reuse or adapt the following submodules:\\n{% for submodule in representative_submodules %}\\n{{ submodule }}\\n{% endfor %}\")\n",
    "    updated_prompt = template.render(representative_submodules=representative_submodules)\n",
    "    return updated_prompt\n",
    "\n",
    "def update_code_by_all_past_results(current_results, past_results):\n",
    "    \"\"\"\n",
    "    Update the current result in combination with the historical results.\n",
    "\n",
    "    current_results: dict The results of the current round.\n",
    "    past_results: dict The results of the historical rounds.\n",
    "\n",
    "    Return the updated results.\n",
    "    \"\"\"\n",
    "    for problem_id, past_result in past_results.items():\n",
    "        if problem_id in current_results:\n",
    "            curr_result = current_results[problem_id]\n",
    "            \n",
    "            if not any(curr_result) and any(past_result):\n",
    "                current_results[problem_id] = past_result\n",
    "        elif any(past_result):\n",
    "            current_results[problem_id] = past_result\n",
    "    \n",
    "    return current_results\n",
    "\n",
    "def cot(query: str, generated_code: str, retriever: VectorStoreRetriever, num_rounds: int = 2, max_attempts: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Use the vector database to query and perform multiple rounds of cot generation.\n",
    "\n",
    "    query: str The query problem.\n",
    "    generated_code: str The code module for reasoning.\n",
    "    retriever: VectorStoreRetriever The retriever used for the query.\n",
    "    num_rounds: int The number of rounds of cot generation.\n",
    "    max_attempts: int The maximum number of generation attempts.\n",
    "\n",
    "    Return the final cot solution.\n",
    "    \"\"\"\n",
    "    prompt1 = PromptTemplate(template=cot_prompt_template, input_variables=[\"context\", \"question\", \"generated_code\"])\n",
    "    prompt2 = PromptTemplate(template=cot_prompt_template_2, input_variables=[\"question\", \"submodules\"])\n",
    "\n",
    "    cot_chain_1 = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\"), \"generated_code\": itemgetter(\"generated_code\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"), generated_code=itemgetter(\"generated_code\"))\n",
    "        | {\"response\": prompt1 | llm2 | StrOutputParser(), \"context\": itemgetter(\"context\"), \"generated_code\": itemgetter(\"generated_code\")}\n",
    "    )\n",
    "    \n",
    "    cot_chain_2 = (\n",
    "        {\"question\": itemgetter(\"question\"), \"submodules\": itemgetter(\"submodules\")}\n",
    "        | RunnablePassthrough.assign(question=itemgetter(\"question\"), submodules=itemgetter(\"submodules\"))\n",
    "        | {\"response\": prompt2 | llm2 | StrOutputParser()}\n",
    "    )\n",
    "    \n",
    "    past_results = {}\n",
    "    representative_submodules_list = []\n",
    "    \n",
    "    for round_idx in tqdm(range(num_rounds), desc=\"Processing rounds\"):\n",
    "        logging.info(f\"Starting round {round_idx}\")\n",
    "        \n",
    "        responses = []\n",
    "        attempts = 0\n",
    "        \n",
    "        while len(responses) < 3 and attempts < max_attempts:\n",
    "            if round_idx == 0:\n",
    "                result = cot_chain_1.invoke({\"question\": query, \"generated_code\": generated_code})\n",
    "            else:\n",
    "                cot_prompt_with_submodules = update_cot_prompt_with_submodules(result[\"response\"], representative_submodules)\n",
    "                result = cot_chain_2.invoke({\"question\": query, \"submodules\": cot_prompt_with_submodules})\n",
    "            \n",
    "            submodules = extract_submodules_from_code(result[\"response\"])\n",
    "            responses.extend(submodules)\n",
    "            attempts += 1\n",
    "        \n",
    "        if not responses:\n",
    "            logging.error(f\"No submodules extracted in round {round_idx} after {max_attempts} attempts.\")\n",
    "            raise ValueError(f\"No submodules extracted in round {round_idx} after {max_attempts} attempts.\")\n",
    "        \n",
    "        embedded_samples = embed_samples(responses)\n",
    "        representative_submodules = extract_representative_submodules(responses)\n",
    "        print(f\"Representative submodules of round {round_idx} extraction: {representative_submodules}\")\n",
    "        \n",
    "        past_results[round_idx] = {\"submodules\": representative_submodules, \"embedded_samples\": embedded_samples}\n",
    "        past_results = update_code_by_all_past_results(past_results, past_results)\n",
    "        representative_submodules_list.extend(representative_submodules)\n",
    "\n",
    "    return representative_submodules_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CFG_part1\n",
    "import ast\n",
    "import astor\n",
    "import graphviz as gv\n",
    "\n",
    "\n",
    "class Block(object):\n",
    "    \"\"\"\n",
    "    Basic block in a control flow graph.\n",
    "\n",
    "    Contains a list of statements executed in a program without any control\n",
    "    jumps. A block of statements is exited through one of its exits. Exits are\n",
    "    a list of Links that represent control flow jumps.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = [\"id\", \"statements\", \"func_calls\", \"predecessors\", \"exits\"]\n",
    "\n",
    "    def __init__(self, id):\n",
    "        # Id of the block.\n",
    "        self.id = id\n",
    "        # Statements in the block.\n",
    "        self.statements = []\n",
    "        # Calls to functions inside the block (represents context switches to\n",
    "        # some functions' CFGs).\n",
    "        self.func_calls = []\n",
    "        # Links to predecessors in a control flow graph.\n",
    "        self.predecessors = []\n",
    "        # Links to the next blocks in a control flow graph.\n",
    "        self.exits = []\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.statements:\n",
    "            return \"block:{}@{}\".format(self.id, self.at())\n",
    "        return \"empty block:{}\".format(self.id)\n",
    "\n",
    "    def __repr__(self):\n",
    "        txt = \"{} with {} exits\".format(str(self), len(self.exits))\n",
    "        if self.statements:\n",
    "            txt += \", body=[\"\n",
    "            txt += \", \".join([ast.dump(node) for node in self.statements])\n",
    "            txt += \"]\"\n",
    "        return txt\n",
    "\n",
    "    def at(self):\n",
    "        \"\"\"\n",
    "        Get the line number of the first statement of the block in the program.\n",
    "        \"\"\"\n",
    "        if self.statements and self.statements[0].lineno >= 0:\n",
    "            return self.statements[0].lineno\n",
    "        return None\n",
    "    \n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        Get the line number of the first statement of the block in the program.\n",
    "        \"\"\"\n",
    "        if self.statements and self.statements[-1].lineno >= 0:\n",
    "            return self.statements[-1].lineno\n",
    "        return None\n",
    "\n",
    "    def is_empty(self):\n",
    "        \"\"\"\n",
    "        Check if the block is empty.\n",
    "\n",
    "        Returns:\n",
    "            A boolean indicating if the block is empty (True) or not (False).\n",
    "        \"\"\"\n",
    "        return len(self.statements) == 0\n",
    "\n",
    "    def get_source(self):\n",
    "        \"\"\"\n",
    "        Get a string containing the Python source code corresponding to the\n",
    "        statements in the block.\n",
    "\n",
    "        Returns:\n",
    "            A string containing the source code of the statements.\n",
    "        \"\"\"\n",
    "        src = \"\"\n",
    "        for statement in self.statements:\n",
    "            if type(statement) in [ast.If, ast.For, ast.While]:\n",
    "                src += (astor.to_source(statement)).split('\\n')[0] + \"\\n\"\n",
    "            elif type(statement) == ast.FunctionDef or\\\n",
    "                 type(statement) == ast.AsyncFunctionDef:\n",
    "                src += (astor.to_source(statement)).split('\\n')[0] + \"...\\n\"\n",
    "            else:\n",
    "                src += astor.to_source(statement)\n",
    "        return src\n",
    "\n",
    "    def get_calls(self):\n",
    "        \"\"\"\n",
    "        Get a string containing the calls to other functions inside the block.\n",
    "\n",
    "        Returns:\n",
    "            A string containing the names of the functions called inside the\n",
    "            block.\n",
    "        \"\"\"\n",
    "        txt = \"\"\n",
    "        for func_name in self.func_calls:\n",
    "            txt += func_name + '\\n'\n",
    "        return txt\n",
    "\n",
    "\n",
    "class Link(object):\n",
    "    \"\"\"\n",
    "    Link between blocks in a control flow graph.\n",
    "\n",
    "    Represents a control flow jump between two blocks. Contains an exitcase in\n",
    "    the form of an expression, representing the case in which the associated\n",
    "    control jump is made.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = [\"source\", \"target\", \"exitcase\"]\n",
    "\n",
    "    def __init__(self, source, target, exitcase=None):\n",
    "        assert type(source) == Block, \"Source of a link must be a block\"\n",
    "        assert type(target) == Block, \"Target of a link must be a block\"\n",
    "        # Block from which the control flow jump was made.\n",
    "        self.source = source\n",
    "        # Target block of the control flow jump.\n",
    "        self.target = target\n",
    "        # 'Case' leading to a control flow jump through this link.\n",
    "        self.exitcase = exitcase\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"link from {} to {}\".format(str(self.source), str(self.target))\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.exitcase is not None:\n",
    "            return \"{}, with exitcase {}\".format(str(self),\n",
    "                                                 ast.dump(self.exitcase))\n",
    "        return str(self)\n",
    "\n",
    "    def get_exitcase(self):\n",
    "        \"\"\"\n",
    "        Get a string containing the Python source code corresponding to the\n",
    "        exitcase of the Link.\n",
    "\n",
    "        Returns:\n",
    "            A string containing the source code.\n",
    "        \"\"\"\n",
    "        if self.exitcase:\n",
    "            return astor.to_source(self.exitcase)\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "class CFG(object):\n",
    "    \"\"\"\n",
    "    Control flow graph (CFG).\n",
    "\n",
    "    A control flow graph is composed of basic blocks and links between them\n",
    "    representing control flow jumps. It has a unique entry block and several\n",
    "    possible 'final' blocks (blocks with no exits representing the end of the\n",
    "    CFG).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, asynchr=False):\n",
    "        assert type(name) == str, \"Name of a CFG must be a string\"\n",
    "        assert type(asynchr) == bool, \"Async must be a boolean value\"\n",
    "        # Name of the function or module being represented.\n",
    "        self.name = name\n",
    "        # Type of function represented by the CFG (sync or async). A Python\n",
    "        # program is considered as a synchronous function (main).\n",
    "        self.asynchr = asynchr\n",
    "        # Entry block of the CFG.\n",
    "        self.entryblock = None\n",
    "        # Final blocks of the CFG.\n",
    "        self.finalblocks = []\n",
    "        # Sub-CFGs for functions defined inside the current CFG.\n",
    "        self.functioncfgs = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"CFG for {}\".format(self.name)\n",
    "\n",
    "    def _visit_blocks(self, graph, block, visited=[], calls=True):\n",
    "        # Don't visit blocks twice.\n",
    "        if block.id in visited:\n",
    "            return\n",
    "\n",
    "        nodelabel = block.get_source()\n",
    "\n",
    "        graph.node(str(block.id), label=nodelabel)\n",
    "        visited.append(block.id)\n",
    "\n",
    "        # Show the block's function calls in a node.\n",
    "        if calls and block.func_calls:\n",
    "            calls_node = str(block.id)+\"_calls\"\n",
    "            calls_label = block.get_calls().strip()\n",
    "            graph.node(calls_node, label=calls_label,\n",
    "                       _attributes={'shape': 'box'})\n",
    "            graph.edge(str(block.id), calls_node, label=\"calls\",\n",
    "                       _attributes={'style': 'dashed'})\n",
    "\n",
    "        # Recursively visit all the blocks of the CFG.\n",
    "        for exit in block.exits:\n",
    "            self._visit_blocks(graph, exit.target, visited, calls=calls)\n",
    "            edgelabel = exit.get_exitcase().strip()\n",
    "            graph.edge(str(block.id), str(exit.target.id), label=edgelabel)\n",
    "\n",
    "    def _build_visual(self, format='pdf', calls=True):\n",
    "        graph = gv.Digraph(name='cluster'+self.name, format=format,\n",
    "                           graph_attr={'label': self.name})\n",
    "        self._visit_blocks(graph, self.entryblock, visited=[], calls=calls)\n",
    "\n",
    "        # Build the subgraphs for the function definitions in the CFG and add\n",
    "        # them to the graph.\n",
    "        for subcfg in self.functioncfgs:\n",
    "            subgraph = self.functioncfgs[subcfg]._build_visual(format=format,\n",
    "                                                               calls=calls)\n",
    "            graph.subgraph(subgraph)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def build_visual(self, filepath, format, calls=True, show=True):\n",
    "        \"\"\"\n",
    "        Build a visualisation of the CFG with graphviz and output it in a DOT\n",
    "        file.\n",
    "\n",
    "        Args:\n",
    "            filename: The name of the output file in which the visualisation\n",
    "                      must be saved.\n",
    "            format: The format to use for the output file (PDF, ...).\n",
    "            show: A boolean indicating whether to automatically open the output\n",
    "                  file after building the visualisation.\n",
    "        \"\"\"\n",
    "        graph = self._build_visual(format, calls)\n",
    "        graph.render(filepath, view=show)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Generator that yields all the blocks in the current graph, then\n",
    "        recursively yields from any sub graphs\n",
    "        \"\"\"\n",
    "        visited = set()\n",
    "        to_visit = [self.entryblock]\n",
    "\n",
    "        while to_visit:\n",
    "            block = to_visit.pop(0)\n",
    "            visited.add(block)\n",
    "            for exit_ in block.exits:\n",
    "                if exit_.target in visited or exit_.target in to_visit:\n",
    "                    continue\n",
    "                to_visit.append(exit_.target)\n",
    "            yield block\n",
    "\n",
    "        for subcfg in self.functioncfgs.values():\n",
    "            yield from subcfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CFG_part2\n",
    "import ast\n",
    "import sys\n",
    "\n",
    "\n",
    "def is_py38_or_higher():\n",
    "    if sys.version_info.major == 3 and sys.version_info.minor >= 8:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "NAMECONSTANT_TYPE = ast.Constant if is_py38_or_higher() else ast.NameConstant\n",
    "\n",
    "\n",
    "def invert(node):\n",
    "    \"\"\"\n",
    "    Invert the operation in an ast node object (get its negation).\n",
    "\n",
    "    Args:\n",
    "        node: An ast node object.\n",
    "\n",
    "    Returns:\n",
    "        An ast node object containing the inverse (negation) of the input node.\n",
    "    \"\"\"\n",
    "    inverse = {ast.Eq: ast.NotEq,\n",
    "               ast.NotEq: ast.Eq,\n",
    "               ast.Lt: ast.GtE,\n",
    "               ast.LtE: ast.Gt,\n",
    "               ast.Gt: ast.LtE,\n",
    "               ast.GtE: ast.Lt,\n",
    "               ast.Is: ast.IsNot,\n",
    "               ast.IsNot: ast.Is,\n",
    "               ast.In: ast.NotIn,\n",
    "               ast.NotIn: ast.In}\n",
    "\n",
    "    if type(node) == ast.Compare:\n",
    "        op = type(node.ops[0])\n",
    "        inverse_node = ast.Compare(left=node.left, ops=[inverse[op]()],\n",
    "                                   comparators=node.comparators)\n",
    "    elif isinstance(node, ast.BinOp) and type(node.op) in inverse:\n",
    "        op = type(node.op)\n",
    "        inverse_node = ast.BinOp(node.left, inverse[op](), node.right)\n",
    "    elif type(node) == NAMECONSTANT_TYPE and node.value in [True, False]:\n",
    "        inverse_node = NAMECONSTANT_TYPE(value=not node.value)\n",
    "    else:\n",
    "        inverse_node = ast.UnaryOp(op=ast.Not(), operand=node)\n",
    "\n",
    "    return inverse_node\n",
    "\n",
    "\n",
    "def merge_exitcases(exit1, exit2):\n",
    "    \"\"\"\n",
    "    Merge the exitcases of two Links.\n",
    "\n",
    "    Args:\n",
    "        exit1: The exitcase of a Link object.\n",
    "        exit2: Another exitcase to merge with exit1.\n",
    "\n",
    "    Returns:\n",
    "        The merged exitcases.\n",
    "    \"\"\"\n",
    "    if exit1:\n",
    "        if exit2:\n",
    "            return ast.BoolOp(ast.And(), values=[exit1, exit2])\n",
    "        return exit1\n",
    "    return exit2\n",
    "\n",
    "\n",
    "class CFGBuilder(ast.NodeVisitor):\n",
    "    \"\"\"\n",
    "    Control flow graph builder.\n",
    "\n",
    "    A control flow graph builder is an ast.NodeVisitor that can walk through\n",
    "    a program's AST and iteratively build the corresponding CFG.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, separate=False):\n",
    "        super().__init__()\n",
    "        self.after_loop_block_stack = []\n",
    "        self.curr_loop_guard_stack = []\n",
    "        self.current_block = None\n",
    "        self.separate_node_blocks = separate\n",
    "\n",
    "    # ---------- CFG building methods ---------- #\n",
    "    def build(self, name, tree, asynchr=False, entry_id=0):\n",
    "        \"\"\"\n",
    "        Build a CFG from an AST.\n",
    "\n",
    "        Args:\n",
    "            name: The name of the CFG being built.\n",
    "            tree: The root of the AST from which the CFG must be built.\n",
    "            async: Boolean indicating whether the CFG being built represents an\n",
    "                   asynchronous function or not. When the CFG of a Python\n",
    "                   program is being built, it is considered like a synchronous\n",
    "                   'main' function.\n",
    "            entry_id: Value for the id of the entry block of the CFG.\n",
    "\n",
    "        Returns:\n",
    "            The CFG produced from the AST.\n",
    "        \"\"\"\n",
    "        self.cfg = CFG(name, asynchr=asynchr)\n",
    "        # Tracking of the current block while building the CFG.\n",
    "        self.current_id = entry_id\n",
    "        self.current_block = self.new_block()\n",
    "        self.cfg.entryblock = self.current_block\n",
    "        # Actual building of the CFG is done here.\n",
    "        self.visit(tree)\n",
    "        self.clean_cfg(self.cfg.entryblock)\n",
    "        return self.cfg\n",
    "\n",
    "    def build_from_src(self, name, src):\n",
    "        \"\"\"\n",
    "        Build a CFG from some Python source code.\n",
    "\n",
    "        Args:\n",
    "            name: The name of the CFG being built.\n",
    "            src: A string containing the source code to build the CFG from.\n",
    "\n",
    "        Returns:\n",
    "            The CFG produced from the source code.\n",
    "        \"\"\"\n",
    "        tree = ast.parse(src, mode='exec')\n",
    "        return self.build(name, tree)\n",
    "\n",
    "    def build_from_file(self, name, filepath):\n",
    "        \"\"\"\n",
    "        Build a CFG from some Python source file.\n",
    "\n",
    "        Args:\n",
    "            name: The name of the CFG being built.\n",
    "            filepath: The path to the file containing the Python source code\n",
    "                      to build the CFG from.\n",
    "\n",
    "        Returns:\n",
    "            The CFG produced from the source file.\n",
    "        \"\"\"\n",
    "        with open(filepath, 'r') as src_file:\n",
    "            src = src_file.read()\n",
    "            return self.build_from_src(name, src)\n",
    "\n",
    "    # ---------- Graph management methods ---------- #\n",
    "    def new_block(self):\n",
    "        \"\"\"\n",
    "        Create a new block with a new id.\n",
    "\n",
    "        Returns:\n",
    "            A Block object with a new unique id.\n",
    "        \"\"\"\n",
    "        self.current_id += 1\n",
    "        return Block(self.current_id)\n",
    "\n",
    "    def add_statement(self, block, statement):\n",
    "        \"\"\"\n",
    "        Add a statement to a block.\n",
    "\n",
    "        Args:\n",
    "            block: A Block object to which a statement must be added.\n",
    "            statement: An AST node representing the statement that must be\n",
    "                       added to the current block.\n",
    "        \"\"\"\n",
    "        block.statements.append(statement)\n",
    "\n",
    "    def add_exit(self, block, nextblock, exitcase=None):\n",
    "        \"\"\"\n",
    "        Add a new exit to a block.\n",
    "\n",
    "        Args:\n",
    "            block: A block to which an exit must be added.\n",
    "            nextblock: The block to which control jumps from the new exit.\n",
    "            exitcase: An AST node representing the 'case' (or condition)\n",
    "                      leading to the exit from the block in the program.\n",
    "        \"\"\"\n",
    "        newlink = Link(block, nextblock, exitcase)\n",
    "        block.exits.append(newlink)\n",
    "        nextblock.predecessors.append(newlink)\n",
    "\n",
    "    def new_loopguard(self):\n",
    "        \"\"\"\n",
    "        Create a new block for a loop's guard if the current block is not\n",
    "        empty. Links the current block to the new loop guard.\n",
    "\n",
    "        Returns:\n",
    "            The block to be used as new loop guard.\n",
    "        \"\"\"\n",
    "        if (self.current_block.is_empty() and\n",
    "                len(self.current_block.exits) == 0):\n",
    "            # If the current block is empty and has no exits, it is used as\n",
    "            # entry block (condition test) for the loop.\n",
    "            loopguard = self.current_block\n",
    "        else:\n",
    "            # Jump to a new block for the loop's guard if the current block\n",
    "            # isn't empty or has exits.\n",
    "            loopguard = self.new_block()\n",
    "            self.add_exit(self.current_block, loopguard)\n",
    "        return loopguard\n",
    "\n",
    "    def new_functionCFG(self, node, asynchr=False):\n",
    "        \"\"\"\n",
    "        Create a new sub-CFG for a function definition and add it to the\n",
    "        function CFGs of the CFG being built.\n",
    "\n",
    "        Args:\n",
    "            node: The AST node containing the function definition.\n",
    "            async: Boolean indicating whether the function for which the CFG is\n",
    "                   being built is asynchronous or not.\n",
    "        \"\"\"\n",
    "        self.current_id += 1\n",
    "        # A new sub-CFG is created for the body of the function definition and\n",
    "        # added to the function CFGs of the current CFG.\n",
    "        func_body = ast.Module(body=node.body)\n",
    "        func_builder = CFGBuilder()\n",
    "        self.cfg.functioncfgs[node.name] = func_builder.build(node.name,\n",
    "                                                              func_body,\n",
    "                                                              asynchr,\n",
    "                                                              self.current_id)\n",
    "        self.current_id = func_builder.current_id + 1\n",
    "\n",
    "    def clean_cfg(self, block, visited=[]):\n",
    "        \"\"\"\n",
    "        Remove the useless (empty) blocks from a CFG.\n",
    "\n",
    "        Args:\n",
    "            block: The block from which to start traversing the CFG to clean\n",
    "                   it.\n",
    "            visited: A list of blocks that already have been visited by\n",
    "                     clean_cfg (recursive function).\n",
    "        \"\"\"\n",
    "        # Don't visit blocks twice.\n",
    "        if block in visited:\n",
    "            return\n",
    "        visited.append(block)\n",
    "\n",
    "        # Empty blocks are removed from the CFG.\n",
    "        if block.is_empty():\n",
    "            for pred in block.predecessors:\n",
    "                for exit in block.exits:\n",
    "                    self.add_exit(pred.source, exit.target,\n",
    "                                  merge_exitcases(pred.exitcase,\n",
    "                                                  exit.exitcase))\n",
    "                    # Check if the exit hasn't yet been removed from\n",
    "                    # the predecessors of the target block.\n",
    "                    if exit in exit.target.predecessors:\n",
    "                        exit.target.predecessors.remove(exit)\n",
    "                # Check if the predecessor hasn't yet been removed from\n",
    "                # the exits of the source block.\n",
    "                if pred in pred.source.exits:\n",
    "                    pred.source.exits.remove(pred)\n",
    "\n",
    "            block.predecessors = []\n",
    "            # as the exits may be modified during the recursive call, it is unsafe to iterate on block.exits\n",
    "            # Created a copy of block.exits before calling clean cfg , and iterate over it instead.\n",
    "            for exit in block.exits[:]:\n",
    "                self.clean_cfg(exit.target, visited)\n",
    "            block.exits = []\n",
    "        else:\n",
    "            for exit in block.exits[:]:\n",
    "                self.clean_cfg(exit.target, visited)\n",
    "\n",
    "    # ---------- AST---------- #\n",
    "    def goto_new_block(self, node):\n",
    "        if self.separate_node_blocks:\n",
    "            newblock = self.new_block()\n",
    "            self.add_exit(self.current_block, newblock)\n",
    "            self.current_block = newblock\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_Expr(self, node):\n",
    "        self.add_statement(self.current_block, node)\n",
    "        self.goto_new_block(node)\n",
    "\n",
    "    def visit_Call(self, node):\n",
    "        def visit_func(node):\n",
    "            if isinstance(node, ast.Name):\n",
    "                return node.id\n",
    "            elif isinstance(node, ast.Attribute):\n",
    "                func_name = visit_func(node.value)\n",
    "                return func_name + \".\" + node.attr\n",
    "            elif isinstance(node, ast.Str):\n",
    "                return node.s\n",
    "            elif isinstance(node, ast.Subscript):\n",
    "                if isinstance(node.value, ast.Name):\n",
    "                    return node.value.id\n",
    "                return \"<unknown_subscript>\"\n",
    "            else:\n",
    "                return type(node).__name__\n",
    "        \"\"\"\n",
    "        def visit_func(node):\n",
    "            if type(node) == ast.Name:\n",
    "                return node.id\n",
    "            elif type(node) == ast.Attribute:\n",
    "                # Recursion on series of calls to attributes.\n",
    "                func_name = visit_func(node.value)\n",
    "                func_name += \".\" + node.attr\n",
    "                return func_name\n",
    "            elif type(node) == ast.Str:\n",
    "                return node.s\n",
    "            elif type(node) == ast.Subscript:\n",
    "                return node.value.id\n",
    "            else:\n",
    "                return type(node).__name__\"\"\"\n",
    "        func = node.func\n",
    "        func_name = visit_func(func)\n",
    "        self.current_block.func_calls.append(func_name)\n",
    "\n",
    "    def visit_Assign(self, node):\n",
    "        self.add_statement(self.current_block, node)\n",
    "        self.goto_new_block(node)\n",
    "\n",
    "    def visit_AnnAssign(self, node):\n",
    "        self.add_statement(self.current_block, node)\n",
    "        self.goto_new_block(node)\n",
    "\n",
    "    def visit_AugAssign(self, node):\n",
    "        self.add_statement(self.current_block, node)\n",
    "        self.goto_new_block(node)\n",
    "\n",
    "    def visit_Raise(self, node):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def visit_Assert(self, node):\n",
    "        self.add_statement(self.current_block, node)\n",
    "        # New block for the case in which the assertion 'fails'.\n",
    "        failblock = self.new_block()\n",
    "        self.add_exit(self.current_block, failblock, invert(node.test))\n",
    "        # If the assertion fails, the current flow ends, so the fail block is a\n",
    "        # final block of the CFG.\n",
    "        self.cfg.finalblocks.append(failblock)\n",
    "        # If the assertion is True, continue the flow of the program.\n",
    "        successblock = self.new_block()\n",
    "        self.add_exit(self.current_block, successblock, node.test)\n",
    "        self.current_block = successblock\n",
    "        self.goto_new_block(node)\n",
    "\n",
    "    def visit_If(self, node):\n",
    "        # Add the If statement at the end of the current block.\n",
    "        self.add_statement(self.current_block, node)\n",
    "\n",
    "        # Create a new block for the body of the if.\n",
    "        if_block = self.new_block()\n",
    "        self.add_exit(self.current_block, if_block, node.test)\n",
    "\n",
    "        # Create a block for the code after the if-else.\n",
    "        afterif_block = self.new_block()\n",
    "\n",
    "        # New block for the body of the else if there is an else clause.\n",
    "        if len(node.orelse) != 0:\n",
    "            else_block = self.new_block()\n",
    "            self.add_exit(self.current_block, else_block, invert(node.test))\n",
    "            self.current_block = else_block\n",
    "            # Visit the children in the body of the else to populate the block.\n",
    "            for child in node.orelse:\n",
    "                self.visit(child)\n",
    "            # If encountered a break, exit will have already been added\n",
    "            if not self.current_block.exits:\n",
    "                self.add_exit(self.current_block, afterif_block)\n",
    "        else:\n",
    "            self.add_exit(self.current_block, afterif_block, invert(node.test))\n",
    "\n",
    "        # Visit children to populate the if block.\n",
    "        self.current_block = if_block\n",
    "        for child in node.body:\n",
    "            self.visit(child)\n",
    "        if not self.current_block.exits:\n",
    "            self.add_exit(self.current_block, afterif_block)\n",
    "\n",
    "        # Continue building the CFG in the after-if block.\n",
    "        self.current_block = afterif_block\n",
    "\n",
    "    def visit_While(self, node):\n",
    "        loop_guard = self.new_loopguard()\n",
    "        self.current_block = loop_guard\n",
    "        self.add_statement(self.current_block, node)\n",
    "        self.curr_loop_guard_stack.append(loop_guard)\n",
    "        # New block for the case where the test in the while is True.\n",
    "        while_block = self.new_block()\n",
    "        self.add_exit(self.current_block, while_block, node.test)\n",
    "\n",
    "        # New block for the case where the test in the while is False.\n",
    "        afterwhile_block = self.new_block()\n",
    "        self.after_loop_block_stack.append(afterwhile_block)\n",
    "        inverted_test = invert(node.test)\n",
    "        # Skip shortcut loop edge if while True:\n",
    "        if not (isinstance(inverted_test, NAMECONSTANT_TYPE) and\n",
    "                inverted_test.value is False):\n",
    "            self.add_exit(self.current_block, afterwhile_block, inverted_test)\n",
    "\n",
    "        # Populate the while block.\n",
    "        self.current_block = while_block\n",
    "        for child in node.body:\n",
    "            self.visit(child)\n",
    "        if not self.current_block.exits:\n",
    "            # Did not encounter a break statement, loop back\n",
    "            self.add_exit(self.current_block, loop_guard)\n",
    "\n",
    "        # Continue building the CFG in the after-while block.\n",
    "        self.current_block = afterwhile_block\n",
    "        self.after_loop_block_stack.pop()\n",
    "        self.curr_loop_guard_stack.pop()\n",
    "\n",
    "    def visit_For(self, node):\n",
    "        loop_guard = self.new_loopguard()\n",
    "        self.current_block = loop_guard\n",
    "        self.add_statement(self.current_block, node)\n",
    "        self.curr_loop_guard_stack.append(loop_guard)\n",
    "        # New block for the body of the for-loop.\n",
    "        for_block = self.new_block()\n",
    "        self.add_exit(self.current_block, for_block, node.iter)\n",
    "\n",
    "        # Block of code after the for loop.\n",
    "        afterfor_block = self.new_block()\n",
    "        self.add_exit(self.current_block, afterfor_block)\n",
    "        self.after_loop_block_stack.append(afterfor_block)\n",
    "        self.current_block = for_block\n",
    "\n",
    "        # Populate the body of the for loop.\n",
    "        for child in node.body:\n",
    "            self.visit(child)\n",
    "        if not self.current_block.exits:\n",
    "            # Did not encounter a break\n",
    "            self.add_exit(self.current_block, loop_guard)\n",
    "\n",
    "        # Continue building the CFG in the after-for block.\n",
    "        self.current_block = afterfor_block\n",
    "        # Popping the current after loop stack,taking care of errors in case of nested for loops\n",
    "        self.after_loop_block_stack.pop()\n",
    "        self.curr_loop_guard_stack.pop()\n",
    "\n",
    "    def visit_Break(self, node):\n",
    "        assert len(self.after_loop_block_stack), \"Found break not inside loop\"\n",
    "        self.add_exit(self.current_block, self.after_loop_block_stack[-1])\n",
    "\n",
    "    def visit_Continue(self, node):\n",
    "        assert len(self.curr_loop_guard_stack), \"Found continue outside loop\"\n",
    "        self.add_exit(self.current_block, self.curr_loop_guard_stack[-1])\n",
    "\n",
    "    def visit_Import(self, node):\n",
    "        self.add_statement(self.current_block, node)\n",
    "\n",
    "    def visit_ImportFrom(self, node):\n",
    "        self.add_statement(self.current_block, node)\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        self.add_statement(self.current_block, node)\n",
    "        self.new_functionCFG(node, asynchr=False)\n",
    "\n",
    "    def visit_AsyncFunctionDef(self, node):\n",
    "        self.add_statement(self.current_block, node)\n",
    "        self.new_functionCFG(node, asynchr=True)\n",
    "\n",
    "    def visit_Await(self, node):\n",
    "        afterawait_block = self.new_block()\n",
    "        self.add_exit(self.current_block, afterawait_block)\n",
    "        self.goto_new_block(node)\n",
    "        self.current_block = afterawait_block\n",
    "\n",
    "    def visit_Return(self, node):\n",
    "        self.add_statement(self.current_block, node)\n",
    "        self.cfg.finalblocks.append(self.current_block)\n",
    "        # Continue in a new block but without any jump to it -> all code after\n",
    "        # the return statement will not be included in the CFG.\n",
    "        self.current_block = self.new_block()\n",
    "\n",
    "    def visit_Yield(self, node):\n",
    "        self.cfg.asynchr = True\n",
    "        afteryield_block = self.new_block()\n",
    "        self.add_exit(self.current_block, afteryield_block)\n",
    "        self.current_block = afteryield_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CFG_part3\n",
    "def generate_cfg_from_code(source_code):\n",
    "\n",
    "    try:\n",
    "\n",
    "        parsed_ast = ast.parse(source_code)\n",
    "        cfg_builder = CFGBuilder()\n",
    "        cfg = cfg_builder.build(name='<string>',tree=parsed_ast, asynchr=False, entry_id=0)\n",
    "        cfg_data = extract_cfg_data(cfg)\n",
    "        return cfg_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while generating CFG: {e}\")\n",
    "        return None\n",
    "def extract_cfg_data(cfg):\n",
    "\n",
    "    cfg_content = {\n",
    "        \"blocks\": [],\n",
    "        \"edges\": [],\n",
    "        \"func_calls\": []\n",
    "    }\n",
    "\n",
    "    for block in cfg:\n",
    "        block_info = {\n",
    "            \"block_id\": block.id,\n",
    "            \"statements\": [ast.dump(statement) for statement in block.statements],\n",
    "            \"func_calls\": block.func_calls\n",
    "        }\n",
    "        cfg_content[\"blocks\"].append(block_info)\n",
    "\n",
    "        for exit in block.exits:\n",
    "            # Get the string form of exitcase\n",
    "            exit_case_str = exit.get_exitcase() if hasattr(exit, 'get_exitcase') else ''\n",
    "            edge_info = {\n",
    "                \"from\": block.id,\n",
    "                \"to\": exit.target.id,\n",
    "                \"condition\": exit_case_str\n",
    "            }\n",
    "            cfg_content[\"edges\"].append(edge_info)\n",
    "\n",
    "    return cfg_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"\n",
    "### Task Description:\n",
    "Evaluate the quality of the following generated code. Please provide detailed feedback and score according to the scoring rules.\n",
    "1. Write detailed feedback and strictly follow the given scoring criteria to evaluate the quality of the answer, focusing on the following aspects:\n",
    "- Functionality: Does the code correctly implement the required functions?\n",
    "- Parameter handling: Does the code accurately handle and utilize the specified parameters (e.g., \"thickness\", \"angle\")?\n",
    "- Output format: Does the code output the results in the correct format (e.g., JSON with specific keys)?\n",
    "- Code structure and readability: Is the code clearly structured and easy to read and understand?\n",
    "- Format: Does the code follow the starter code? (If the starter code is empty, ignore this item)\n",
    "2. After writing the feedback, give a score between 1 and 10 according to the detailed scoring criteria.\n",
    "3. The output should be in the following format: \"Feedback: {{Write feedback for the criteria}} Result: {{Integer between 1 and 10}}\"\n",
    "4. Please do not generate any additional descriptions. Always include the result in the output.\n",
    "\n",
    "### Tasks for generating code:\n",
    "{task_example}\n",
    "\n",
    "### Starting code:\n",
    "{starter_code}\n",
    "\n",
    "### Generated code:\n",
    "{generated_code}\n",
    "\n",
    "### Scoring criteria:\n",
    "[Evaluate the code based on the following aspects:\n",
    "- 1-2 points: The code is completely incorrect and does not meet any requirements. There are serious problems in functionality, parameter handling, output format, code structure and readability, format, etc.\n",
    "- 3-4 points: Most of the code is wrong, and there are major problems in functionality, parameter handling or output format. The code structure and readability are poor, and the format does not meet the requirements.\n",
    "- 5-6 points: The code is partially correct, some aspects are correctly implemented, but there are obvious problems. There are some problems in functionality, parameter handling, output format, code structure and readability, format, etc.\n",
    "- 7-8 points: Most of the code is correct, implements most of the functions required for the task, parameter handling is reasonable, and the output format is correct. The code structure and readability are good, and the format basically meets the requirements. But there may be some minor problems with the overall code.\n",
    "- 9-10points: The code is completely correct, all functions required by the task are accurately implemented, the parameters are processed correctly, and the output is in line with expectations. The code structure is clear, easy to read and understand, and the format fully meets the requirements. ]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair assessor of language models.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def balanced_position_calibration(scores, k):\n",
    "    \"\"\"Calculating the weighted average score using Balanced Position Calibration\"\"\"\n",
    "    weights = [(k - i) / k for i in range(k)]\n",
    "    weighted_scores = [score * weight for score, weight in zip(scores, weights)]\n",
    "    weighted_average = sum(weighted_scores) / sum(weights)\n",
    "    return weighted_average\n",
    "\n",
    "def summarize_feedback(feedbacks):\n",
    "    \"\"\"Summarize the main modification suggestions from multiple feedbacks.\"\"\"\n",
    "    combined_feedback = \"\\n\".join(feedbacks)\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following feedback into key points that highlight the necessary changes and improvements:\n",
    "    {combined_feedback}\n",
    "    \"\"\"\n",
    "    message = HumanMessage(content=prompt.strip())\n",
    "    summary = eval_chat_model.invoke([message])  \n",
    "    \n",
    "    return summary.content.strip()\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    generated_code_file: str,\n",
    "    starter_code,\n",
    "    eval_chat_model,\n",
    "    evaluation_prompt_template,\n",
    "    task_example: str,\n",
    "    k: int = 5,  # Set the number of multiple scoring\n",
    "    results_file: str = \"evaluation_results.json\"\n",
    ") -> None:\n",
    "    \"\"\"Evaluate the generated code and save the results to a JSON file.\"\"\"\n",
    "\n",
    "    def load_code(file_path: str) -> str:\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, \"r\") as f:\n",
    "                return f.read()\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"file not found: {file_path}\")\n",
    "\n",
    "    # Load the initial code and generated code\n",
    "    starter_code = starter_code\n",
    "    generated_code = load_code(generated_code_file)\n",
    "\n",
    "    scores = []\n",
    "    feedbacks = []\n",
    "\n",
    "    # Call the model multiple times for scoring\n",
    "    for _ in range(k):\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            task_example=task_example,\n",
    "            starter_code=starter_code,\n",
    "            generated_code=generated_code\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "\n",
    "        # Try parsing feedback and ratings from model responses\n",
    "        try:\n",
    "            match = re.search(r\"Feedback:\\s*(.*?)\\s*Result:\\s*(\\d+)\", eval_result.content, re.DOTALL | re.IGNORECASE)\n",
    "            if match:\n",
    "                feedback = match.group(1).strip()\n",
    "                score = int(match.group(2).strip())\n",
    "                feedbacks.append(feedback)\n",
    "                scores.append(score)\n",
    "            else:\n",
    "                raise ValueError(\"Unable to parse model response, please ensure the response is well formatted.\")\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            feedbacks.append(\"Unable to extract feedback\")\n",
    "            scores.append(0)\n",
    "\n",
    "    # Calculating the weighted average score using Balanced Position Calibration\n",
    "    final_score = balanced_position_calibration(scores, k)\n",
    "\n",
    "    # Use summarization method to generate final feedback\n",
    "    final_feedback = summarize_feedback(feedbacks)\n",
    "\n",
    "    evaluation_result = {\n",
    "        \"task_example\": task_example,\n",
    "        \"starter_code\": starter_code,\n",
    "        \"generated_code\": generated_code,\n",
    "        \"feedback\": final_feedback,\n",
    "        \"score\": final_score\n",
    "    }\n",
    "\n",
    "    # Read existing result file (if it exists)\n",
    "    if os.path.isfile(results_file):\n",
    "        with open(results_file, \"r\") as f:\n",
    "            existing_results = json.load(f)\n",
    "    else:\n",
    "        existing_results = []\n",
    "\n",
    "    existing_results.append(evaluation_result)\n",
    "\n",
    "    with open(results_file, \"w\") as f:\n",
    "        json.dump(existing_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Final Rating: {final_score}\")\n",
    "    print(f\"Final Feedback: {final_feedback}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_code_from_file(file_path: str) -> str:\n",
    "    \"\"\"Read code content from a file.\"\"\"\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"file not found: {file_path}\")\n",
    "\n",
    "def generate_code_with_feedback(output: str, feedback: str = \"\", cfg: str = \"\") -> str:\n",
    "    \"\"\"Generate code and make improvements based on feedback and CFG.\"\"\"\n",
    "\n",
    "    cot_prompt_template_4 = \"\"\"\n",
    "    *Task*\n",
    "    Thoroughly review and improve the provided code based on feedback and control flow graph (CFG). You must address each feedback point in detail and ensure that the code meets the highest quality standards.\n",
    "    *Output*\n",
    "    Code: Revised code, including import statements for necessary modules and functions. And add error handling for each submodule.\n",
    "    (Your output should strictly follow the code file format, not contain other additional natural language, and should be pure code without any code block markers (such as ```python```))\n",
    "    *Input*\n",
    "    Original code: {Output}\n",
    "    Feedback: {feedback}\n",
    "    Control flow graph (CFG): {cfg}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt4 = PromptTemplate(template=cot_prompt_template_4, input_variables=[\"Output\", \"feedback\", \"cfg\"])\n",
    "    cot_chain_4 = (\n",
    "        {\"Output\": itemgetter(\"Output\"), \"feedback\": itemgetter(\"feedback\"), \"cfg\": itemgetter(\"cfg\")}\n",
    "        | RunnablePassthrough.assign(code=itemgetter(\"Output\"), feedback=itemgetter(\"feedback\"), cfg=itemgetter(\"cfg\"))\n",
    "        | {\"response\": prompt4 | llm2 | StrOutputParser()}\n",
    "    )\n",
    "    \n",
    "    result = cot_chain_4.invoke({\"Output\": output, \"feedback\": feedback, \"cfg\": cfg})\n",
    "    solution = result['response']\n",
    " \n",
    "    return solution\n",
    "\n",
    "def evaluate_and_regenerate(\n",
    "    file_path: str,\n",
    "    eval_chat_model,\n",
    "    evaluation_prompt_template,\n",
    "    starter_code: str,\n",
    "    task_example: str,\n",
    "    score_threshold: int = 8,\n",
    "    max_retries: int = 2,\n",
    "    results_file: str = \"evaluation_results.json\"\n",
    ") -> str:\n",
    "    \"\"\"Generate code and evaluate it, and regenerate code using feedback and CFG if the score is below a threshold.\"\"\"\n",
    "    \n",
    "    retry_count = 0\n",
    "    feedback = \"\"\n",
    "    output = load_code_from_file(file_path)\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    while retry_count <= max_retries:\n",
    "        # Reload the code\n",
    "        output = load_code_from_file(file_path)\n",
    "        logging.info(f\"Reload the code: \\n{output}\")\n",
    "\n",
    "        try:\n",
    "            evaluate_answers(\n",
    "                starter_code=starter_code,\n",
    "                generated_code_file=file_path,\n",
    "                eval_chat_model=eval_chat_model,\n",
    "                evaluation_prompt_template=evaluation_prompt_template,\n",
    "                task_example=task_example,\n",
    "                results_file=results_file,\n",
    "                k=5\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while evaluating the code: {e}\")\n",
    "            break\n",
    "    # Read the latest score and feedback from the evaluation result file\n",
    "        try:\n",
    "            with open(results_file, \"r\") as f:\n",
    "                existing_results = json.load(f)\n",
    "            latest_result = existing_results[-1]  \n",
    "            score = int(latest_result[\"score\"])\n",
    "            feedback = latest_result[\"feedback\"]\n",
    "            logging.info(f\"Latest Assessment Results - Rating: {score}, feedback: {feedback}\")\n",
    "        except (FileNotFoundError, json.JSONDecodeError, IndexError) as e:\n",
    "            logging.error(f\"An error occurred while reading the evaluation results file: {e}\")\n",
    "            break\n",
    "\n",
    "        if score >= score_threshold:\n",
    "            logging.info(f\"Code rating is{score}The threshold has been reached.\")\n",
    "            return output\n",
    "        else:\n",
    "            logging.info(f\"Code rating is{score}Below the threshold, an attempt will be made to regenerate code based on the feedback and the CFG.\")\n",
    "            retry_count += 1\n",
    "\n",
    "            # CFG\n",
    "            try:\n",
    "                cfg_code_content = generate_cfg_from_code(output)\n",
    "                if cfg_code_content:\n",
    "                    logging.info(\"Generated CFG content: \\n%s\", cfg_code_content)\n",
    "                else:\n",
    "                    logging.warning(\"CFG generation failed, use normal generation process.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error generating CFG: {e}\")\n",
    "                cfg_code_content = None\n",
    "\n",
    "            #Regenerate code based on feedback and CFG\n",
    "            try:\n",
    "                solution = generate_code_with_feedback(output, feedback, cfg_code_content)\n",
    "                output = solution\n",
    "                logging.info(f\"Regenerated code: \\n{output}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"An error occurred while generating code: {e}\")\n",
    "                break\n",
    "\n",
    "            # Extract the code content and overwrite the original file\n",
    "            code = extract_code(solution)\n",
    "            if not code:\n",
    "                logging.error(\"Unable to extract code content, skipping this retry.\")\n",
    "                break\n",
    "            with open(file_path, \"w\") as f:\n",
    "                f.write(code)\n",
    "            output = code  \n",
    "            logging.info(f\"The overwritten code has been written to the file: {file_path}\")\n",
    "\n",
    "    logging.info(f\"The maximum number of rewrites has been reached {max_retries}\")\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
